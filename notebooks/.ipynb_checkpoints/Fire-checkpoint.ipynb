{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ac0214",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69df2653",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\vikas/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "C:\\Users\\vikas\\Desktop\\ForestFireTracking\\envfire\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "YOLOv5  2024-5-25 Python-3.10.4 torch-2.2.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "custom_YOLOv5s summary: 232 layers, 7246518 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('ultralytics/yolov5', 'custom', '../models/yoloFire.pt')  # force_reload=True to update\n",
    "\n",
    "# Other Parameters\n",
    "pd = 0\n",
    "text = \"emergency\"\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "font_scale = 0.8\n",
    "font_thickness = 1\n",
    "text_color = (255, 255, 255)  # White color\n",
    "pixel_length_meters = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "680f99c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo(im, size=640):\n",
    "    g = (size / max(im.size))  \n",
    "    im = im.resize((int(x * g) for x in im.size), Image.LANCZOS)\n",
    "    results = model(im)\n",
    "    \n",
    "#     results.render()  \n",
    "\n",
    "    result_image = Image.fromarray(results.ims[0])\n",
    "    result_frame = np.array(result_image)\n",
    "\n",
    "    return results, result_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94184d4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "159b96ad",
   "metadata": {},
   "source": [
    "### 1. Area using pixel segmentation\n",
    "- Output of this will be 3 frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3894896",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess1(results, result_frame):\n",
    "    \n",
    "#     black_frame = np.zeros_like(result_frame)\n",
    "    black_frame = np.zeros((result_frame.shape[0] + 30, result_frame.shape[1], 3), dtype=np.uint8)\n",
    "    for box in results.xyxy[0]:\n",
    "        \n",
    "        xmin, ymin, xmax, ymax, _, _ = box\n",
    "        \n",
    "        cv2.rectangle(result_frame, (int(xmin), int(ymin)), (int(xmax), int(ymax)), (10, 255, 0), 2)\n",
    "        cv2.imshow('Result Frame', result_frame)\n",
    "        \n",
    "        # Extract the region of interest (ROI) from the original frame\n",
    "        roi = result_frame[int(ymin)-pd:int(ymax)+pd, int(xmin)-pd:int(xmax)+pd]\n",
    "        \n",
    "        # Convert the ROI to YCbCr color space\n",
    "        roi_ycbcr = cv2.cvtColor(roi, cv2.COLOR_RGB2YCrCb)\n",
    "        \n",
    "        roi_highlighted = np.zeros_like(roi)\n",
    "        for y in range(roi.shape[0]):\n",
    "            for x in range(roi.shape[1]):\n",
    "                Y, Cr, Cb = roi_ycbcr[y, x]\n",
    "                if Y >= Cb and Cr >= Cb and Y <= 150:\n",
    "#                     fire_pixels = fire_pixels + 1\n",
    "                    \n",
    "                    roi_highlighted[y, x] = np.array([0,143,255]) \n",
    "#                     roi_highlighted[y,x] = roi[y, x][::-1]\n",
    "\n",
    "        black_frame[int(ymin)-pd:int(ymax)+pd, int(xmin)-pd:int(xmax)+pd] = roi_highlighted\n",
    "        \n",
    "        area = 0\n",
    "        gray = cv2.cvtColor(black_frame, cv2.COLOR_BGR2GRAY)\n",
    "        ret, thresh = cv2.threshold(gray, 50, 255, cv2.THRESH_BINARY)\n",
    "        \n",
    "        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        # Iterate through each contour\n",
    "        for contour in contours:\n",
    "            # Calculate area of the contour\n",
    "            pixel_area = cv2.contourArea(contour)  # Area in pixels\n",
    "\n",
    "            area += pixel_area\n",
    "            # Draw contour on the original image\n",
    "            cv2.drawContours(black_frame, [contour], -1, (0, 255, 0), 1)\n",
    "        \n",
    "        pr = area*100/(black_frame.shape[0]*black_frame.shape[1])\n",
    "        \n",
    "        cv2.putText(black_frame, \"Area: {}%\".format(round(pr,2)), (20, 370), font, font_scale, text_color, font_thickness, lineType=cv2.LINE_AA)\n",
    "    \n",
    "    return black_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a4381b",
   "metadata": {},
   "source": [
    "### 2. Area using Bounding boxes\n",
    "- This will show 2 frames (Video & Bounding box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8307a0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess2(results, result_frame):\n",
    "    total_area = 0\n",
    "    black_frame = np.zeros((result_frame.shape[0] + 30, result_frame.shape[1], 3), dtype=np.uint8)\n",
    "    for box in results.xyxy[0]:\n",
    "        \n",
    "        xmin, ymin, xmax, ymax, _, _ = box\n",
    "        cv2.rectangle(black_frame, (int(xmin), int(ymin)), (int(xmax), int(ymax)), (10, 255, 0), 1)\n",
    "        \n",
    "        w = xmax - xmin\n",
    "        h = ymax - ymin\n",
    "        \n",
    "        total_area += w * h\n",
    "\n",
    "        # Calculate the percentage of the frame that the average area occupies\n",
    "        pr =  total_area* 100 / (black_frame.shape[0] * black_frame.shape[1])\n",
    "\n",
    "        cv2.putText(black_frame, \"Area: {}%\".format(round(float(pr), 2)), (20, 370), font, font_scale, text_color, font_thickness, lineType=cv2.LINE_AA)\n",
    "\n",
    "    return black_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d1aea06",
   "metadata": {},
   "outputs": [],
   "source": [
    "areas = []\n",
    "# start_frame = 6100  # Change this to the frame number you want to start from\n",
    "\n",
    "cap = cv2.VideoCapture(\"C:/Users/vikas/Downloads/1-Zenmuse_X4S_1.mp4\")\n",
    "# cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "\n",
    "while True:\n",
    "    # Read a frame from the video\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Break the loop if the video is over\n",
    "    if not ret:\n",
    "        print('Error in capturing frames')\n",
    "        break\n",
    "    \n",
    "    # Convert the frame from OpenCV format (BGR) to PIL format (RGB)\n",
    "    frame_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    \n",
    "    # Function Calling\n",
    "    results, result_frame = yolo(frame_pil)\n",
    "    \n",
    "#     black_frame = preprocess1(results, result_frame)\n",
    "    \n",
    "    black_frame = preprocess2(results, result_frame)\n",
    "    \n",
    "    cv2.imshow('YOLOv5 result', cv2.cvtColor(result_frame, cv2.COLOR_RGB2BGR))\n",
    "    \n",
    "    # Detected Area After Pre-processing\n",
    "    cv2.imshow('Preprocess', black_frame)\n",
    "    \n",
    "    # Press q to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e43d9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b4bfa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1c36e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8546d28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8b5171c",
   "metadata": {},
   "source": [
    "### Farneback Method for Optical flow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ec42b75",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "color = [255, 255, 255]\n",
    "avgx = []\n",
    "avgy = []\n",
    "\n",
    "# Clockwise positive direction\n",
    "\n",
    "# Open video file\n",
    "cap = cv2.VideoCapture('../demoVideos/test1 (2).mp4')\n",
    "\n",
    "# cap = cv2.VideoCapture('../DemoVideos/paperFire2.mp4')\n",
    "# cap.set(cv2.CAP_PROP_POS_FRAMES, 1000)\n",
    "\n",
    "# Create random colors for visualizing optical flow tracks\n",
    "# color = np.random.randint(0,255,(100,3))\n",
    "\n",
    "ret, frame1 = cap.read()\n",
    "frame1 = cv2.resize(frame1, (960,540))\n",
    "prvs = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "hsv = np.zeros_like(frame1)\n",
    "hsv[...,1] =  255  # Assigning 255 to Saturation Value in hsv Image\n",
    "\n",
    "# Create a black frame for motion visualization\n",
    "black_frame = np.zeros_like(frame1)\n",
    "\n",
    "frame_count = 1\n",
    "\n",
    "while True:\n",
    "    ret, frame2 = cap.read()\n",
    "    frame2 = cv2.resize(frame2, (960,540))\n",
    "    \n",
    "#     frame_count += 1\n",
    "    \n",
    "    if ret and frame_count:\n",
    "#         frame_count = 0\n",
    "        new = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Calculate Optical Flow\n",
    "        flow = cv2.calcOpticalFlowFarneback(prvs, new, None, \n",
    "                                            0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "\n",
    "        # Get average flow direction\n",
    "        avgx.append(np.mean(flow[..., 0]))\n",
    "        avgy.append(np.mean(flow[..., 1]))\n",
    "        \n",
    "        if len(avgx) > 30: \n",
    "            avg_fx = np.mean(avgx)\n",
    "            avg_fy = np.mean(avgy)\n",
    "            avg_direction_angle = np.arctan2(avg_fy, avg_fx)\n",
    "            avg_direction_degrees = np.degrees(avg_direction_angle)\n",
    "            \n",
    "            # Set text on black_frame\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            cv2.putText(frame2, f\"Direction wrt X+: {avg_direction_degrees:.2f} degrees\", (10, 30), font, 1, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "            \n",
    "            avgx.pop()\n",
    "            avgy.pop()\n",
    "\n",
    "        # Overlay motion visualization onto the original frame\n",
    "        for y in range(0, black_frame.shape[0], 10):\n",
    "            for x in range(0, black_frame.shape[1], 10):\n",
    "                fx, fy = flow[y, x]\n",
    "                cv2.line(black_frame, (x, y), (int(x + fx), int(y + fy)), color, 1)\n",
    "        \n",
    "    \n",
    "        color[0] -= 1\n",
    "        color[1] -= 1\n",
    "\n",
    "        # Display original video and motion visualization\n",
    "        cv2.imshow('Original Video', frame2)\n",
    "        cv2.imshow('Fire Spread Motion', black_frame)\n",
    "    \n",
    "        prvs = new\n",
    "    \n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break   \n",
    "\n",
    "# Release video capture object and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a3b575",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c0f621",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d63c773",
   "metadata": {},
   "source": [
    "### Motion Analysis by subtracting frames!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd7c922f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the video\n",
    "video_path = '../demoVideos/smoke.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "# cap.set(cv2.CAP_PROP_POS_FRAMES, 12000)\n",
    "\n",
    "# Check if the video opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "    exit()\n",
    "\n",
    "# Initialize the background subtractor\n",
    "fgbg = cv2.createBackgroundSubtractorMOG2()\n",
    "\n",
    "while True:\n",
    "    # Read a frame from the video\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.resize(frame, (960,540))\n",
    "\n",
    "    # Check if the frame was read successfully\n",
    "    if not ret:\n",
    "        print(\"Error in capturing frames\")\n",
    "        break\n",
    "    \n",
    "    # Apply background subtraction to detect fire pixels\n",
    "    fgmask = fgbg.apply(frame)\n",
    "    \n",
    "    cv2.imshow('Mask', fgmask)\n",
    "\n",
    "    # Apply thresholding to segment fire regions\n",
    "    _, thresh = cv2.threshold(fgmask, 150, 250, cv2.THRESH_BINARY)\n",
    "    \n",
    "\n",
    "    # Find contours of fire regions\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Draw bounding boxes around fire regions\n",
    "    for contour in contours:\n",
    "        area = cv2.contourArea(contour)\n",
    "        \n",
    "        if area > 500:\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255), 2)\n",
    "\n",
    "    # Display the frame with fire detection\n",
    "    cv2.imshow('Fire Detection', frame)\n",
    "\n",
    "    # Check for user input to exit\n",
    "    if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9985d8e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a62585d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1408d2b",
   "metadata": {},
   "source": [
    "### Direction using Lucas Kanade > You can skip this :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2cddc27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "cap = cv2.VideoCapture('../DemoVideos/forestFire1.mp4')\n",
    "# cap.set(cv2.CAP_PROP_POS_FRAMES, 1000)\n",
    "\n",
    "feature_params = dict(maxCorners=2500, qualityLevel=0.5, minDistance=5, blockSize=2)\n",
    "\n",
    "lk_params = dict(winSize=(40,40), maxLevel=5, criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 20, 0.03))\n",
    "\n",
    "color = np.random.randint(0, 255, (100, 3))\n",
    "\n",
    "ret, old_frame = cap.read()\n",
    "old_gray = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY)\n",
    "p0 = cv2.goodFeaturesToTrack(old_gray, mask=None, **feature_params)\n",
    "\n",
    "mask = np.zeros_like(old_frame)\n",
    "\n",
    "frame_count = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    " \n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    frame_count += 1\n",
    "    \n",
    "    if frame_count % 50 == 0:  \n",
    "        frame_count = 0\n",
    "        \n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Calculate Optical Flow\n",
    "        p1, st, err = cv2.calcOpticalFlowPyrLK(old_gray, gray, p0, None, **lk_params)\n",
    "\n",
    "        # Select good points\n",
    "        good_new = p1[st==1]\n",
    "        good_old = p0[st==1]\n",
    "\n",
    "        # Draw the optical flow tracks\n",
    "        for i, (new, old) in enumerate(zip(good_new, good_old)):\n",
    "            a, b = new.ravel()\n",
    "            c, d = old.ravel()\n",
    "            mask = cv2.line(mask, (int(a), int(b)), (int(c), int(d)), color[i].tolist(), 2)\n",
    "#             frame = cv2.circle(frame, (int(a), int(b)), 5, color[i].tolist(), -1)\n",
    "\n",
    "        # Combine frame with optical flow tracks\n",
    "#         img = cv2.add(frame, mask)\n",
    "        \n",
    "        # Display the result\n",
    "        cv2.imshow('Optical Flow', mask)\n",
    "        \n",
    "        # Update old frame and points for next iteration\n",
    "        old_gray = gray.copy()\n",
    "        p0 = good_new.reshape(-1, 1, 2)\n",
    "        \n",
    "    cv2.imshow('Frame', frame)\n",
    "    \n",
    "    # Check for user input to exit\n",
    "    if cv2.waitKey(100) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b5e88c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0433073",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "83660f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9495c5c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df697e66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85416daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 350.7ms\n",
      "Speed: 72.5ms preprocess, 350.7ms inference, 32.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 89.0ms\n",
      "Speed: 8.0ms preprocess, 89.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 78.1ms\n",
      "Speed: 0.0ms preprocess, 78.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 82.7ms\n",
      "Speed: 0.0ms preprocess, 82.7ms inference, 8.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 77.8ms\n",
      "Speed: 0.0ms preprocess, 77.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 101.9ms\n",
      "Speed: 0.0ms preprocess, 101.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 68.5ms\n",
      "Speed: 0.0ms preprocess, 68.5ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 75.4ms\n",
      "Speed: 0.7ms preprocess, 75.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 83.5ms\n",
      "Speed: 0.0ms preprocess, 83.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 83.8ms\n",
      "Speed: 0.0ms preprocess, 83.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 61.7ms\n",
      "Speed: 12.1ms preprocess, 61.7ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 83.4ms\n",
      "Speed: 0.0ms preprocess, 83.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 73.4ms\n",
      "Speed: 4.9ms preprocess, 73.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 68.7ms\n",
      "Speed: 0.0ms preprocess, 68.7ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 73.2ms\n",
      "Speed: 4.3ms preprocess, 73.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 84.1ms\n",
      "Speed: 0.0ms preprocess, 84.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 82.4ms\n",
      "Speed: 0.0ms preprocess, 82.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 81.5ms\n",
      "Speed: 0.0ms preprocess, 81.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 73.0ms\n",
      "Speed: 0.0ms preprocess, 73.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 74.1ms\n",
      "Speed: 3.5ms preprocess, 74.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 77.9ms\n",
      "Speed: 0.0ms preprocess, 77.9ms inference, 5.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 61.6ms\n",
      "Speed: 0.7ms preprocess, 61.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 67.3ms\n",
      "Speed: 1.1ms preprocess, 67.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 60.4ms\n",
      "Speed: 0.0ms preprocess, 60.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 76.0ms\n",
      "Speed: 6.6ms preprocess, 76.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 72.5ms\n",
      "Speed: 8.5ms preprocess, 72.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 68.6ms\n",
      "Speed: 0.0ms preprocess, 68.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 66.4ms\n",
      "Speed: 0.0ms preprocess, 66.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 83.5ms\n",
      "Speed: 15.2ms preprocess, 83.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 55.2ms\n",
      "Speed: 0.0ms preprocess, 55.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 67.8ms\n",
      "Speed: 0.0ms preprocess, 67.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 65.6ms\n",
      "Speed: 8.2ms preprocess, 65.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 67.3ms\n",
      "Speed: 5.6ms preprocess, 67.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 72.9ms\n",
      "Speed: 0.0ms preprocess, 72.9ms inference, 8.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 67.8ms\n",
      "Speed: 5.5ms preprocess, 67.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 73.2ms\n",
      "Speed: 2.4ms preprocess, 73.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 69.8ms\n",
      "Speed: 6.0ms preprocess, 69.8ms inference, 7.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 72.3ms\n",
      "Speed: 4.5ms preprocess, 72.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 64.0ms\n",
      "Speed: 2.6ms preprocess, 64.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 67.4ms\n",
      "Speed: 4.9ms preprocess, 67.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 63.9ms\n",
      "Speed: 0.0ms preprocess, 63.9ms inference, 13.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 69.4ms\n",
      "Speed: 3.7ms preprocess, 69.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 63.4ms\n",
      "Speed: 7.4ms preprocess, 63.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 67.9ms\n",
      "Speed: 0.0ms preprocess, 67.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 65.2ms\n",
      "Speed: 0.0ms preprocess, 65.2ms inference, 15.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 76.5ms\n",
      "Speed: 2.4ms preprocess, 76.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 74.3ms\n",
      "Speed: 1.7ms preprocess, 74.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 71.5ms\n",
      "Speed: 0.0ms preprocess, 71.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 71.1ms\n",
      "Speed: 5.9ms preprocess, 71.1ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 65.1ms\n",
      "Speed: 0.0ms preprocess, 65.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 78.7ms\n",
      "Speed: 4.8ms preprocess, 78.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 67.1ms\n",
      "Speed: 0.0ms preprocess, 67.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 83.6ms\n",
      "Speed: 11.2ms preprocess, 83.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 81.5ms\n",
      "Speed: 0.0ms preprocess, 81.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 64.2ms\n",
      "Speed: 4.2ms preprocess, 64.2ms inference, 8.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 89.0ms\n",
      "Speed: 0.0ms preprocess, 89.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 77.0ms\n",
      "Speed: 0.0ms preprocess, 77.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 72.0ms\n",
      "Speed: 8.0ms preprocess, 72.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 67.4ms\n",
      "Speed: 6.0ms preprocess, 67.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 73.2ms\n",
      "Speed: 2.5ms preprocess, 73.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 68.4ms\n",
      "Speed: 2.4ms preprocess, 68.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 70.5ms\n",
      "Speed: 0.0ms preprocess, 70.5ms inference, 8.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 56.3ms\n",
      "Speed: 9.0ms preprocess, 56.3ms inference, 8.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 77.0ms\n",
      "Speed: 0.0ms preprocess, 77.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 81.9ms\n",
      "Speed: 0.0ms preprocess, 81.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 75.9ms\n",
      "Speed: 0.0ms preprocess, 75.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 76.5ms\n",
      "Speed: 0.0ms preprocess, 76.5ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 68.4ms\n",
      "Speed: 3.4ms preprocess, 68.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n",
      "\n",
      "0: 384x640 77.5ms\n",
      "Speed: 8.0ms preprocess, 77.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the YOLOv8 model\n",
    "model = YOLO('../models/yoloNano.pt', verbose=False)\n",
    "\n",
    "# Define the video path and starting frame\n",
    "video_path = \"../demoVideos/1-Zenmuse_X4S_Clip2.mp4\"\n",
    "start_frame = 500\n",
    "\n",
    "# Open the video file and set the starting frame\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "\n",
    "# Loop through the video frames\n",
    "while cap.isOpened():\n",
    "    # Read a frame from the video\n",
    "    success, frame = cap.read()\n",
    "\n",
    "    if success:\n",
    "        frame = cv2.resize(frame, (960, 540))\n",
    "        \n",
    "        cnt = 0\n",
    "        # Run YOLOv8 inference on the frame\n",
    "        results = model.track(frame, persist=True)\n",
    "        \n",
    "        \n",
    "\n",
    "        # Extract the detection results\n",
    "        detections = results[0].obb.xywhr.cpu().numpy()  # Bounding box coordinates\n",
    "#         scores = results[0].obb.conf.cpu().numpy()  # Confidence scores\n",
    "#         classes = results[0].obb.cls.cpu().numpy()  # Class indices\n",
    "        ids = results[0].obb.id.int().cpu().numpy()\n",
    "        print(ids)\n",
    "\n",
    "        # Draw custom bounding boxes on the frame\n",
    "#         for i in range(len(detections)):\n",
    "        for box, track_id in zip(detections, ids):\n",
    "            x, y, w, h, r = box\n",
    "#             score = scores[i]\n",
    "#             class_idx = int(classes[i])\n",
    "\n",
    "            # Convert the center (x, y), width, height, and rotation angle into a box format\n",
    "            rect = ((x, y), (w, h), np.degrees(r))\n",
    "            box = cv2.boxPoints(rect)\n",
    "            box = np.intp(box)\n",
    "\n",
    "            # Draw the bounding box\n",
    "#             color = colors[class_idx % len(colors)]\n",
    "#             label = f\"{labels[class_idx % len(labels)]}: {score:.2f}\"\n",
    "            cv2.polylines(frame, [box], True, (0,0,255), 2)\n",
    "            cv2.putText(frame, str(track_id), (int(box[0][0]), int(box[0][1]) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,0), 2)\n",
    "\n",
    "        # Display the annotated frame\n",
    "        cv2.imshow(\"YOLOv8 Inference\", frame)\n",
    "\n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close the display window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0b7dbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Release the video capture object and close the display window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f437a6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f5b928",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3062a741",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envfire",
   "language": "python",
   "name": "envfire"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
