{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08a66cc0",
   "metadata": {},
   "source": [
    "### 3 Steps\n",
    "- First step is to camera is stable or not \n",
    "- second step is to detect ROI by connected component analysis\n",
    "- Block based approach is used 1 and 2 step\n",
    "- Using K-temporal info decide it's smoke or not\n",
    "br"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e40e70ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Open video file\n",
    "cap = cv2.VideoCapture('../demoVideos/test (1).mp4')\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.resize(frame, (960,540))\n",
    "    \n",
    "    if not ret:\n",
    "        break\n",
    "        \n",
    "    cv2.imshow('Frames', frame)\n",
    "    \n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break   \n",
    "\n",
    "# Release video capture object and clse all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7f3285",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2257fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1ad7c3d",
   "metadata": {},
   "source": [
    "## Research Paper Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c6669fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WX and WY (Sub-block size):\n",
    "# These parameters define the size of the sub-blocks used for analysis.\n",
    "# Larger values (e.g., 32x32) will make the algorithm \n",
    "# less sensitive to small movements efficient for computation\n",
    "\n",
    "def camera_motion_detection(BG, BI, WX=20, WY=20, Th=0.5):\n",
    "    # Calculate BD(t, x, y) as per equation (1)\n",
    "    # Absolute block mean difference\n",
    "    BD = np.abs(BG - BI)\n",
    "    \n",
    "    # Calculate sub-block average AD(i, j) as per equation (2)\n",
    "    M, N = BD.shape\n",
    "    i_blocks = M // WY\n",
    "    j_blocks = N // WX\n",
    "    \n",
    "    AD = np.zeros((i_blocks, j_blocks))\n",
    "    \n",
    "    for i in range(i_blocks):\n",
    "        for j in range(j_blocks):\n",
    "            y_start, y_end = i*WY, (i+1)*WY\n",
    "            x_start, x_end = j*WX, (j+1)*WX\n",
    "            AD[i, j] = np.mean(BD[y_start:y_end, x_start:x_end])\n",
    "    \n",
    "    # Count sub-blocks with AD(i, j) > Th\n",
    "    blocks_above_threshold = np.sum(AD > Th)\n",
    "    total_blocks = i_blocks * j_blocks\n",
    "    \n",
    "    # Check if more than half of sub-blocks are above threshold\n",
    "    if blocks_above_threshold > 0.9*total_blocks:\n",
    "        return BI.copy(), BD, True\n",
    "    else:\n",
    "        # No significant camera motion, segment blobs\n",
    "        return BG, BD, False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5c2b7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba6ad807",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blob_segmentation(BD, Prev_BB, Th=1):\n",
    "    # Create binary image BB(t,x,y) as per equation (3)\n",
    "    BB = (BD >= Th).astype(int)\n",
    "    \n",
    "    # Apply bitwise AND with previous frame's BB if available (equation 4)\n",
    "    if prev_BB is not None:\n",
    "        BB = np.logical_and(BB, prev_BB).astype(int)\n",
    "\n",
    "#     # Apply morphological operations to remove small blobs\n",
    "#     dilation_kernel = np.ones((2,2), dtype=bool)\n",
    "#     erosion_kernel = np.ones((1,1), dtype=bool)\n",
    "    \n",
    "    BB = binary_dilation(BB)\n",
    "    BB = binary_erosion(BB)\n",
    "    \n",
    "    # Apply connected component algorithm\n",
    "    labeled_array, num_features = label(BB)\n",
    "    \n",
    "    return BB, labeled_array, num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1e24c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b3abe1e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from scipy.ndimage import label, binary_erosion, binary_dilation\n",
    "\n",
    "# Open video file\n",
    "BLOCK_SIZE = 4\n",
    "prev_BB = None\n",
    "cap = cv2.VideoCapture('../demoVideos/smoke.mp4')\n",
    "\n",
    "# Initial frame processing\n",
    "ret, frame = cap.read()\n",
    "frame = cv2.resize(frame, (960,540))\n",
    "yuv_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2YUV)\n",
    "y_channel = yuv_frame[:, :, 0].astype(np.float32)\n",
    "BG = cv2.boxFilter(y_channel, ddepth=-1, ksize=(BLOCK_SIZE, BLOCK_SIZE))\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if not ret:\n",
    "        break\n",
    "        \n",
    "    # Sequential Frame processing\n",
    "    frame = cv2.resize(frame, (960,540))\n",
    "    yuv_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2YUV)\n",
    "    y_channel = yuv_frame[:, :, 0].astype(np.float32)\n",
    "    BI = cv2.boxFilter(y_channel, ddepth=-1, ksize=(BLOCK_SIZE, BLOCK_SIZE))\n",
    "\n",
    "    new_BG, BD, is_moving = camera_motion_detection(BG, BI)\n",
    "    BG = new_BG\n",
    "    \n",
    "    cv2.imshow('Difference', BD)\n",
    "\n",
    "    if not is_moving:\n",
    "        BB, BLOBS, N = blob_segmentation(BD, Prev_BB = prev_BB)\n",
    "        blobs_visual = (BLOBS.astype(np.float32) / BLOBS.max()) * 255\n",
    "        cv2.imshow('BB Visualization', blobs_visual)\n",
    "#         print('NO of BLOBs: ',N)\n",
    "        prev_BB = BB\n",
    "    else:\n",
    "        prev_BB = None\n",
    "    \n",
    "    cv2.putText(frame, \"motion\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0) if is_moving else (0, 0, 255), 2)\n",
    "    \n",
    "    cv2.imshow('Frames', frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break   \n",
    "\n",
    "# Release video capture object and clse all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f7f97c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e61c35b",
   "metadata": {},
   "source": [
    "### Smoke-detection approach using spatial and temporal analyses, which is based on the block-processing technique. This method analyzes energy-based and color-based features within the spatial, temporal, and spatial-temporal domains, before all the proposed features are combined using an SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8594effd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3260b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7c2c378",
   "metadata": {},
   "source": [
    "## High Magnitude Optical Flow Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f8873b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def detect_smoke_farneback(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    ret, first_frame = cap.read()\n",
    "    first_frame = cv2.resize(first_frame, (960,540))\n",
    "    prev_gray = cv2.cvtColor(first_frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        frame = cv2.resize(frame, (960,540))\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Apply Gaussian blur to reduce noise\n",
    "        frame_blur = cv2.GaussianBlur(frame, (5, 5), 0)\n",
    "        gray = cv2.cvtColor(frame_blur, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Calculate dense optical flow using Farneback method\n",
    "        flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None,\n",
    "                                            pyr_scale=0.5, levels=3, winsize=15, \n",
    "                                            iterations=3, poly_n=5, poly_sigma=1.1, flags=0)\n",
    "        \n",
    "        # Compute magnitude and angle of 2D vector\n",
    "        magnitude, angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "        \n",
    "        # Normalize magnitude for visualization\n",
    "        magnitude_norm = cv2.normalize(magnitude, None, 0, 255, cv2.NORM_MINMAX)\n",
    "        \n",
    "        # Create a fresh mask for each frame\n",
    "        mask = np.zeros_like(frame)\n",
    "        mask[..., 1] = 255  # Set green channel to 255\n",
    "        \n",
    "        # Threshold for high magnitude\n",
    "        high_magnitude_mask = magnitude > 0.1\n",
    "        mask[..., 0][high_magnitude_mask] = angle[high_magnitude_mask] * 180 / np.pi / 2\n",
    "        mask[..., 2][high_magnitude_mask] = magnitude_norm[high_magnitude_mask]\n",
    "        \n",
    "        # Convert HSV to RGB (BGR) color representation\n",
    "        rgb = cv2.cvtColor(mask, cv2.COLOR_HSV2BGR)\n",
    "        result = cv2.addWeighted(frame, 1, rgb, 2, 0)\n",
    "     \n",
    "        cv2.imshow('Smoke Detection Farneback', result)\n",
    "        \n",
    "        k = cv2.waitKey(30) & 0xff\n",
    "        if k == 27:  # Press 'Esc' to exit\n",
    "            break\n",
    "        \n",
    "        # Update previous frame\n",
    "        prev_gray = gray\n",
    "    \n",
    "    cv2.destroyAllWindows()\n",
    "    cap.release()\n",
    "\n",
    "# Usage\n",
    "video_path = '../demoVideos/smoke.mp4'\n",
    "detect_smoke_farneback(video_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "757b1c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2589ba1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e14a56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "020c4649",
   "metadata": {},
   "source": [
    "### MOG Background Subtraction & Optical Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f8016f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vikas\\Desktop\\ForestFireTracking\\envfire\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\vikas\\Desktop\\ForestFireTracking\\envfire\\lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "avgx = []\n",
    "avgy = []\n",
    "\n",
    "def detect_smoke_farneback_mog2(video_path):\n",
    "    start_frame = 2000  \n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "#     cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "    \n",
    "    fgbg = cv2.createBackgroundSubtractorMOG2(detectShadows=True)\n",
    "    ret, first_frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to read video\")\n",
    "        return\n",
    "    \n",
    "    first_frame = cv2.resize(first_frame, (960, 540))\n",
    "    prev_gray = cv2.cvtColor(first_frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        frame = cv2.resize(frame, (960, 540))\n",
    "        \n",
    "        # Background subtraction\n",
    "        fgmask = fgbg.apply(frame)\n",
    "        \n",
    "        # Apply morphological operations to reduce noise\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "        fgmask = cv2.morphologyEx(fgmask, cv2.MORPH_CLOSE, kernel)\n",
    "        fgmask = cv2.morphologyEx(fgmask, cv2.MORPH_OPEN, kernel)\n",
    "        \n",
    "        # Calculate Farneback optical flow\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "        \n",
    "        # Get average flow direction\n",
    "        avgx.append(np.mean(flow[fgmask > 0, 0]))\n",
    "        avgy.append(np.mean(flow[fgmask > 0, 1]))\n",
    "\n",
    "        if len(avgx) > 30: \n",
    "            avg_fx = np.mean(avgx)\n",
    "            avg_fy = np.mean(avgy)\n",
    "            avg_direction_angle = np.arctan2(avg_fy, avg_fx)\n",
    "            avg_direction_degrees = np.degrees(avg_direction_angle)\n",
    "\n",
    "            # Set text on black_frame\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            cv2.putText(frame, f\"Direction wrt X+: {avg_direction_degrees:.2f} degrees\", (10, 30), font, 1, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "            avgx.pop(0)\n",
    "            avgy.pop(0)\n",
    "        \n",
    "        \n",
    "        step = 15\n",
    "        for y in range(0, frame.shape[0], step):\n",
    "            for x in range(0, frame.shape[1], step):\n",
    "                fx, fy = flow[y, x]\n",
    "                if fgmask[y, x] > 0:  # Only draw arrows on detected regions\n",
    "                    cv2.arrowedLine(frame, (x, y), (int(x + fx), int(y + fy)), (0, 255, 0), 2, tipLength=4)\n",
    "        \n",
    "        # Display the result\n",
    "        cv2.imshow('Smoke Detection (Farneback + MOG2)', frame)\n",
    "        cv2.imshow('Foreground Mask', fgmask)\n",
    "        \n",
    "        if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "            break\n",
    "        \n",
    "        # Update previous frame\n",
    "        prev_gray = gray\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "video_path = '../demoVideos/smoke.mp4'\n",
    "detect_smoke_farneback_mog2(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2390d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930007a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd89738",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "341deaf5",
   "metadata": {},
   "source": [
    "## K-Means Clustering & Thresholding + Optical Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e3bdcf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "def kmeans_clustering_lab(image, n_clusters=3):\n",
    "    lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2Lab)\n",
    "    reshaped_image = lab_image.reshape((-1, 3))\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42).fit(reshaped_image)\n",
    "    clustered_image = kmeans.labels_.reshape(image.shape[:2])\n",
    "    return clustered_image\n",
    "\n",
    "def iterative_threshold_segmentation(image):\n",
    "    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    lower_bound = np.array([0, 50, 0], dtype=np.uint8)\n",
    "    upper_bound = np.array([180, 200, 255], dtype=np.uint8)\n",
    "    \n",
    "    thresholded_image = cv2.inRange(hsv_image, lower_bound, upper_bound)\n",
    "    return thresholded_image\n",
    "\n",
    "def merge_segmentations(kmeans_result, threshold_result):\n",
    "    merged_result = np.logical_and(kmeans_result, threshold_result)\n",
    "    return merged_result.astype(np.uint8) * 255\n",
    "\n",
    "def shen_filter(image):\n",
    "    # Apply Gaussian filter as an approximation for Shen filter\n",
    "    return gaussian_filter(image, sigma=1)\n",
    "\n",
    "def remove_noise(image):\n",
    "    # Use connected components to remove small regions\n",
    "    num_labels, labels_im = cv2.connectedComponents(image)\n",
    "    sizes = np.bincount(labels_im.ravel())\n",
    "    mask_sizes = sizes > 1000  # Filter out small regions\n",
    "    mask_sizes[0] = 0  # Background is set to False\n",
    "    cleaned_image = mask_sizes[labels_im]\n",
    "    return cleaned_image.astype(np.uint8) * 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6c2c191e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    ret, first_frame = cap.read()\n",
    "    first_frame = cv2.resize(first_frame, (640, 480))\n",
    "    prev_gray = cv2.cvtColor(first_frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        frame = cv2.resize(frame, (640, 480))\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Step 1: K-means Clustering in Lab Color Space\n",
    "        kmeans_result = kmeans_clustering_lab(frame, n_clusters=3)\n",
    "\n",
    "        # Step 2: Iterative Threshold Segmentation in HSV Color Space\n",
    "        threshold_result = iterative_threshold_segmentation(frame)\n",
    "\n",
    "        # Step 3: Merge the Results\n",
    "        merged_result = merge_segmentations(kmeans_result, threshold_result)\n",
    "\n",
    "        # Step 4: Denoising\n",
    "        filtered_result = shen_filter(merged_result)\n",
    "        final_result = remove_noise(filtered_result)\n",
    "        invert_mask = cv2.bitwise_not(final_result)\n",
    "        \n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "        \n",
    "        # Get average flow direction\n",
    "        avgx.append(np.mean(flow[invert_mask > 0, 0]))\n",
    "        avgy.append(np.mean(flow[invert_mask > 0, 1]))\n",
    "\n",
    "        if len(avgx) > 30: \n",
    "            avg_fx = np.mean(avgx)\n",
    "            avg_fy = np.mean(avgy)\n",
    "            avg_direction_angle = np.arctan2(avg_fy, avg_fx)\n",
    "            avg_direction_degrees = np.degrees(avg_direction_angle)\n",
    "\n",
    "            # Set text on black_frame\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            cv2.putText(frame, f\"Direction wrt X+: {avg_direction_degrees:.2f} degrees\", (10, 30), font, 1, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "            avgx.pop(0)\n",
    "            avgy.pop(0)\n",
    "        \n",
    "        \n",
    "        step = 15\n",
    "        for y in range(0, frame.shape[0], step):\n",
    "            for x in range(0, frame.shape[1], step):\n",
    "                fx, fy = flow[y, x]\n",
    "                if invert_mask[y, x] > 0:  # Only draw arrows on detected regions\n",
    "                    cv2.arrowedLine(frame, (x, y), (int(x + fx), int(y + fy)), (0, 255, 0), 2, tipLength=4)\n",
    "        \n",
    "        # Display the result\n",
    "        cv2.imshow('Smoke Detection (Farneback + Thresholding + KMeans)', frame)\n",
    "        cv2.imshow('Foreground Mask', invert_mask)\n",
    "        \n",
    "        if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "            break\n",
    "        \n",
    "        # Update previous frame\n",
    "        prev_gray = gray\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Run the smoke segmentation on a sample video\n",
    "video_path = '../demoVideos/smoke.mp4'\n",
    "process_video(video_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4dc185db",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43630938",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f19c58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envfire",
   "language": "python",
   "name": "envfire"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
